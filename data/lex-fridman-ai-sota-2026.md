# State of AI in 2026: LLMs, Coding, Scaling Laws, China, Agents, GPUs, AGI

**Источник:** [Lex Fridman Podcast #490](https://lexfridman.com/ai-sota-2026-transcript)  
**Гости:** Nathan Lambert & Sebastian Raschka

Разговор о состоянии искусственного интеллекта в 2026: прорывы, LLM, кодинг, законы масштабирования, открытые и закрытые модели, pre-training / mid-training / post-training, RLVR, AGI, робототехника, будущее цивилизации.

---

## Transcript (фрагмент)

### Introduction

**Lex Fridman (00:00:00)** The following is a conversation all about the state of the art in artificial intelligence, including some of the exciting technical breakthroughs and developments in AI that happened over the past year, and some of the interesting things we think might happen this upcoming year. At times, it does get super technical, but we do try to make sure that it remains accessible to folks outside the field without ever dumbing it down. It is a great honor and pleasure to be able to do this kind of episode with two of my favorite people in the AI community, Sebastian Raschka and Nathan Lambert. They are both widely respected machine learning researchers and engineers who also happen to be great communicators, educators, writers, and X posters.

**Lex Fridman (00:00:51)** Sebastian is the author of two books I highly recommend for beginners and experts alike. First is Build a Large Language Model from Scratch, and Build a Reasoning Model from Scratch. I truly believe in the machine learning and computer science world, the best way to learn and understand something is to build it yourself from scratch. Nathan is the post-training lead at the Allen Institute for AI, and author of the definitive book on reinforcement learning from human feedback. Both of them have great X accounts, great Substacks. Sebastian has courses on YouTube, Nathan has a podcast. And everyone should absolutely follow all of those. This is the Lex Fridman podcast.

### China vs US: Who wins the AI race?

**Lex Fridman (00:01:57)** So I think one useful lens to look at all this through is the so-called DeepSeek moment. This happened about a year ago in January 2025, when the open weight Chinese company DeepSeek released DeepSeek R1 that I think it's fair to say surprised everyone with near or at state-of-the-art performance, with allegedly much less compute for much cheaper. And from then to today, the AI competition has gotten insane, both on the research level and the product level. It's just been accelerating.

**Sebastian Raschka (00:02:53)** So winning is a very broad term. I would say you mentioned the DeepSeek moment, and I do think DeepSeek is definitely winning the hearts of the people who work on open weight models because they share these as open models. Winning, I think, has multiple timescales to it. We have today, we have next year, we have in ten years. One thing I know for sure is that I don't think nowadays, in 2026, that there will be any company having access to a technology that no other company has access to. And that is mainly because researchers are frequently changing jobs, changing labs. They rotate. So I don't think there will be a clear winner in terms of technology access. However, I do think the differentiating factor will be budget and hardware constraints. I don't think the ideas will be proprietary, but rather the resources that are needed to implement them. And so I don't currently see a winner-takes-all scenario. I can't see that at the moment.

*(В эпизоде также: ChatGPT vs Claude vs Gemini vs Grok, Best AI for coding, Open Source vs Closed Source LLMs, Transformers, AI Scaling Laws, Pre-training / Mid-training / Post-training, RLVR, Continual learning, Long context, Robotics, Timeline to AGI, Will AI replace programmers?, How AI will make money?, Future of NVIDIA, Future of human civilization.)*

---

**Полный транскрипт:** https://lexfridman.com/ai-sota-2026-transcript
